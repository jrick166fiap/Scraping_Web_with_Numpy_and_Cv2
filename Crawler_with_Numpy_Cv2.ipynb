{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 PUBLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Web with Numpy and Cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def LerArquivo(subdiretorio):\n",
    "    x = datetime.datetime.now()\n",
    "    data = x.strftime(\"%Y%m%d\")\n",
    "    arq = open(subdiretorio,'r', encoding='utf-8')\n",
    "    #print(arq.read())\n",
    "    lista = []\n",
    "    for item in arq:\n",
    "        #print(item)\n",
    "        lista.append(str(item).strip())\n",
    "    print(lista)\n",
    "    arq.close()\n",
    "    return lista\n",
    "    \n",
    "def gravarNoArquivoUrl(n1,n2,diretorio):\n",
    "    x = datetime.datetime.now()\n",
    "    data = x.strftime('%d-%m-%Y_%H')\n",
    "    arquivo = open(diretorio+'\\\\catalogo_resultado_'+ data+'.csv', 'a',encoding='utf-8')\n",
    "    arquivo.write('%s;%s\\n' %(n1,n2))\n",
    "    arquivo.flush()\n",
    "    arquivo.close()\n",
    "    \n",
    "def gerarImagensConcatenadas():\n",
    "    #ref(tks) https://note.nkmk.me/en/python-opencv-hconcat-vconcat-np-tile/\n",
    "    path = '.\\Jornais'\n",
    "    diretorio = os.listdir(path)\n",
    "    for a in diretorio:\n",
    "        pathSub = path + '\\\\'+ a\n",
    "        #subdiretorio =  os.listdir(pathSub)\n",
    "        for b in os.listdir(pathSub):\n",
    "            if 'Images' in b:\n",
    "                subdiretorio2 = pathSub + '\\\\' + b\n",
    "                #print(subdiretorio2)\n",
    "                list_array = []\n",
    "                list_array_temp = []\n",
    "                contador = 0\n",
    "                for images in os.listdir(subdiretorio2):\n",
    "                    print(pathSub)\n",
    "                    image = subdiretorio2.replace('\\\\','//', 3) +'/'+ images\n",
    "                    #print(subdiretorio2)\n",
    "                    #ler imagens\n",
    "                    imm1 = cv2.imread(image)\n",
    "                    #deixar imagens no mesmo padrao\n",
    "                    imm2 = cv2.resize(imm1, (1280, 720))\n",
    "                    #Entender\n",
    "                    im1_s = cv2.resize(imm2, dsize=(0, 0), fx=0.5, fy=0.5)\n",
    "                    #adicionar no array\n",
    "                    list_array_temp.append(im1_s)\n",
    "                    contador += 1\n",
    "\n",
    "                    #quando o resto do total de imagens for 0\n",
    "                    #imagina uma matriz 3x3 onde y vai ser o total de imagens\n",
    "                    if contador == 3 and len(list_array_temp) == 3:\n",
    "                        list_array.append(list_array_temp)\n",
    "                        list_array_temp = []\n",
    "                        contador = 0\n",
    "                        controle = True\n",
    "                        temp = False\n",
    "                    #quando o resto do total de imagens for maior que zero\n",
    "                    #imagina uma matriz 2x2 onde y vai ser o total de imagens\n",
    "                    elif contador == 2 and (len(os.listdir(subdiretorio2)) % 3 > 0):\n",
    "                        list_array.append(list_array_temp)\n",
    "                        list_array_temp = []\n",
    "                        contador = 0\n",
    "                    #bug sempre vai ignorar a ultima imagem da lista\n",
    "\n",
    "                    #controle = False  \n",
    "                concatenacao = cv2.vconcat([cv2.hconcat(im_list_h) for im_list_h in list_array])\n",
    "                cv2.imwrite(pathSub +'\\\\'+ 'final.jpg', concatenacao)\n",
    "    \n",
    "\n",
    "def getImages(link, pagina_base,diretorio):\n",
    "    \n",
    "    try:\n",
    "        dados_pagina = requests.get(link, timeout=30)\n",
    "    except:\n",
    "        print('Erro ao abrir a página' + pagina)\n",
    "\n",
    "    sopa = BeautifulSoup(dados_pagina.content,'lxml')\n",
    "\n",
    "    images = sopa.find_all('img')\n",
    "    links = set()\n",
    "\n",
    "    for image in images:\n",
    "        if ('src' in image.attrs):\n",
    "            #url = str(image.get('src'))\n",
    "            url = urljoin(pagina_base, str(image.get('src')))\n",
    "            title = str(image.get('title')).lower()\n",
    "            #print(title)\n",
    "            if '.jpg' in url or '.jpeg' in url or '.png' in url:\n",
    "                if url.__contains__('covid-19') or url.__contains__('coronavírus') or url.__contains__('coronavirus') or url.__contains__('covid')  or title.__contains__('covid-19') or title.__contains__('coronavírus') or title.__contains__('coronavirus') or title.__contains__('covid'):\n",
    "                    print(url)\n",
    "                    links.add(url)\n",
    "                \n",
    "    for a in links: \n",
    "        try:\n",
    "            j = requests.get(a, timeout=30)\n",
    "            hash = random.getrandbits(128)\n",
    "            \n",
    "            #Estudar forma e evoluir isso\n",
    "            #remover_acentos = a.split('/')[-1].split('.')[0].lower()\n",
    "            #removeSpecialChars = remover_acentos.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+áàãíìóòõç\"})\n",
    "            with open(diretorio+'\\\\Images\\\\'+str(hash)+'.png', 'wb') as f:\n",
    "                f.write(j.content)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def crawl(paginas, profundidade,diretorio):\n",
    "    x = datetime.datetime.now()\n",
    "    data = x.strftime(\"%Y/%m/%d\")\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    user_agent = {'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    for i in range(profundidade):\n",
    "        novas_paginas = set()\n",
    "        for pagina in paginas:\n",
    "            http = urllib3.PoolManager(10,headers=user_agent)\n",
    "            try:\n",
    "                dados_pagina = http.request('GET',pagina)\n",
    "            except:\n",
    "                print('Erro ao abrir a página' + pagina)\n",
    "                continue\n",
    "            \n",
    "            sopa = BeautifulSoup(dados_pagina.data,'lxml')\n",
    "            \n",
    "            noticiaslinks = set()\n",
    "            links = sopa.find_all('a')\n",
    "            contador = 1\n",
    "            for link in links:\n",
    "\n",
    "                if ('title' in link.attrs):\n",
    "                    title = str(link.get('title').lower())\n",
    "                    url = urljoin(pagina, str(link.get('href')))\n",
    "\n",
    "                    if title.__contains__('coronavirus') or title.__contains__('coronavírus') or title.__contains__('COVID-19') or title.__contains__('covid'):\n",
    "                        if url[-1] == '/':\n",
    "                            tamanho = len(url)\n",
    "                            url = url[:tamanho-1]\n",
    "                        noticiaslinks.add(url)\n",
    "\n",
    "                if ('href' in link.attrs):\n",
    "                    url = urljoin(pagina, str(link.get('href')))\n",
    "                        \n",
    "                    if url.find(\"'\") != -1:\n",
    "                        continue\n",
    "                    url = url.split('#')[0]                    \n",
    "                    if url[0:4] == 'http' or url[0:5] == 'https':\n",
    "                        novas_paginas.add(url)\n",
    "                        if url.__contains__('coronavirus') or url.__contains__('coronavírus') or url.__contains__('COVID-19') or url.__contains__('covid'):\n",
    "                            if url[-1] == '/':\n",
    "                                tamanho = len(url)\n",
    "                                url = url[:tamanho-1]\n",
    "                            noticiaslinks.add(url)\n",
    "            \n",
    "                    contador = contador + 1\n",
    "            for link in noticiaslinks:\n",
    "                gravarNoArquivoUrl(pagina,link,diretorio)\n",
    "                getImages(link,pagina,diretorio)\n",
    "                    \n",
    "            print(contador)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    #Start\n",
    "    #Pasta base\n",
    "    path = '.\\Jornais'\n",
    "    diretorio = os.listdir(path)\n",
    "    for a in diretorio:\n",
    "        pathSub = path + '\\\\'+ a\n",
    "        subdiretorio =  os.listdir(pathSub)\n",
    "        for file in subdiretorio:\n",
    "            if 'txt' in file:\n",
    "                fullpath = pathSub+'\\\\'+file\n",
    "                sites = LerArquivo(fullpath)\n",
    "                print(fullpath)\n",
    "                crawl(sites,1,pathSub)\n",
    "    gerarImagensConcatenadas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Base conhecimento inicio\n",
    "#import cv2\n",
    "#import numpy as np\n",
    "#img1 = cv2.imread('./Images/alcool.png')\n",
    "#im1 = cv2.resize(img1, (200, 200)) \n",
    "#img2 = cv2.imread('./Images/alcool.png')\n",
    "#im2 = cv2.resize(img2, (200, 200))\n",
    "#img3 = cv2.imread('./Images/alcool.png')\n",
    "#im3 = cv2.resize(img3, (200, 200))\n",
    "#vis = np.concatenate((im1), axis=1)\n",
    "#cv2.imshow('image', vis)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#cv2.imwrite('out.png', vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
